{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20Newsgroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comp 5 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4891 documents\n",
      "5 categories\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((array([0, 1]), array([3909,  982])), (4891, 13446))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "data = fetch_20newsgroups(subset='all', shuffle=False, remove=('headers', 'footers', 'quotes'), categories=['comp.graphics', \n",
    "        'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x'])\n",
    "X, y = data.data, data.target\n",
    "print(\"%d documents\" % len(data.filenames))\n",
    "print(\"%d categories\" % len(data.target_names))\n",
    "\n",
    "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8, stop_words='english')\n",
    "vectorizer = Pipeline([\n",
    "    ('vect', CountVectorizer(**vectorizer_params)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "])\n",
    "X = vectorizer.fit_transform(X)\n",
    "msq = y == 2\n",
    "y[msq] = 1\n",
    "y[~msq] = 0\n",
    "np.unique(y, return_counts=True), X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ids_train = []\n",
    "ids_test = []\n",
    "\n",
    "for i in range(20):\n",
    "    id_train, id_test = train_test_split(np.arange(len(y)), test_size=.3)\n",
    "    ids_train.append(id_train)\n",
    "    ids_test.append(id_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([88, 12]))\n",
      "(array([0, 1]), array([85, 15]))\n",
      "(array([0, 1]), array([75, 25]))\n",
      "(array([0, 1]), array([79, 21]))\n",
      "(array([0, 1]), array([80, 20]))\n",
      "(array([0, 1]), array([84, 16]))\n",
      "(array([0, 1]), array([80, 20]))\n",
      "(array([0, 1]), array([84, 16]))\n",
      "(array([0, 1]), array([88, 12]))\n",
      "(array([0, 1]), array([85, 15]))\n",
      "(array([0, 1]), array([81, 19]))\n",
      "(array([0, 1]), array([80, 20]))\n",
      "(array([0, 1]), array([83, 17]))\n",
      "(array([0, 1]), array([80, 20]))\n",
      "(array([0, 1]), array([83, 17]))\n",
      "(array([0, 1]), array([84, 16]))\n",
      "(array([0, 1]), array([78, 22]))\n",
      "(array([0, 1]), array([77, 23]))\n",
      "(array([0, 1]), array([84, 16]))\n",
      "(array([0, 1]), array([88, 12]))\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(np.unique(y[ids_train[i][:100]], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('splits/trsplitcomp5', ids_train)\n",
    "np.savez_compressed('splits/tstsplitcomp5', ids_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseball-hockey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1993 documents\n",
      "2 categories\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((array([0, 1]), array([994, 999])), (1993, 5724))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "data = fetch_20newsgroups(subset='all', shuffle=False, remove=('headers', 'footers', 'quotes'), categories=['rec.sport.baseball', 'rec.sport.hockey'])\n",
    "X, y = data.data, data.target\n",
    "print(\"%d documents\" % len(data.filenames))\n",
    "print(\"%d categories\" % len(data.target_names))\n",
    "\n",
    "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8, stop_words='english')\n",
    "vectorizer = Pipeline([\n",
    "    ('vect', CountVectorizer(**vectorizer_params)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "])\n",
    "X = vectorizer.fit_transform(X)\n",
    "np.unique(y, return_counts=True), X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ids_train = []\n",
    "ids_test = []\n",
    "\n",
    "for i in range(20):\n",
    "    id_train, id_test = train_test_split(np.arange(len(y)), test_size=.3)\n",
    "    ids_train.append(id_train)\n",
    "    ids_test.append(id_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([5, 5]))\n",
      "(array([0, 1]), array([4, 6]))\n",
      "(array([0, 1]), array([6, 4]))\n",
      "(array([0, 1]), array([4, 6]))\n",
      "(array([0, 1]), array([5, 5]))\n",
      "(array([0, 1]), array([2, 8]))\n",
      "(array([0, 1]), array([6, 4]))\n",
      "(array([0, 1]), array([5, 5]))\n",
      "(array([0, 1]), array([5, 5]))\n",
      "(array([0, 1]), array([5, 5]))\n",
      "(array([0, 1]), array([5, 5]))\n",
      "(array([0, 1]), array([7, 3]))\n",
      "(array([0, 1]), array([4, 6]))\n",
      "(array([0, 1]), array([3, 7]))\n",
      "(array([0, 1]), array([6, 4]))\n",
      "(array([0, 1]), array([7, 3]))\n",
      "(array([0, 1]), array([6, 4]))\n",
      "(array([0, 1]), array([4, 6]))\n",
      "(array([0, 1]), array([3, 7]))\n",
      "(array([0, 1]), array([5, 5]))\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(np.unique(y[ids_train[i][:10]], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('splits/trsplitbase_hock', ids_train)\n",
    "np.savez_compressed('splits/tstsplitbase_hock', ids_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pc-mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1945 documents\n",
      "2 categories\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((array([0, 1]), array([982, 963])), (1945, 3868))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "data = fetch_20newsgroups(subset='all', shuffle=False, remove=('headers', 'footers', 'quotes'), categories=['comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware'])\n",
    "X, y = data.data, data.target\n",
    "print(\"%d documents\" % len(data.filenames))\n",
    "print(\"%d categories\" % len(data.target_names))\n",
    "\n",
    "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8, stop_words='english')\n",
    "vectorizer = Pipeline([\n",
    "    ('vect', CountVectorizer(**vectorizer_params)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "])\n",
    "X = vectorizer.fit_transform(X)\n",
    "np.unique(y, return_counts=True), X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ids_train = []\n",
    "ids_test = []\n",
    "\n",
    "for i in range(20):\n",
    "    id_train, id_test = train_test_split(np.arange(len(y)), test_size=.3)\n",
    "    ids_train.append(id_train)\n",
    "    ids_test.append(id_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([6, 4]))\n",
      "(array([0, 1]), array([4, 6]))\n",
      "(array([0, 1]), array([4, 6]))\n",
      "(array([0, 1]), array([6, 4]))\n",
      "(array([0, 1]), array([7, 3]))\n",
      "(array([0, 1]), array([7, 3]))\n",
      "(array([0, 1]), array([3, 7]))\n",
      "(array([0, 1]), array([2, 8]))\n",
      "(array([0, 1]), array([4, 6]))\n",
      "(array([0, 1]), array([4, 6]))\n",
      "(array([0, 1]), array([4, 6]))\n",
      "(array([0, 1]), array([5, 5]))\n",
      "(array([0, 1]), array([6, 4]))\n",
      "(array([0, 1]), array([6, 4]))\n",
      "(array([0, 1]), array([6, 4]))\n",
      "(array([0, 1]), array([6, 4]))\n",
      "(array([0, 1]), array([7, 3]))\n",
      "(array([0, 1]), array([7, 3]))\n",
      "(array([0, 1]), array([3, 7]))\n",
      "(array([0, 1]), array([7, 3]))\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(np.unique(y[ids_train[i][:10]], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('splits/trsplitpc_mac', ids_train)\n",
    "np.savez_compressed('splits/tstsplitpc_mac', ids_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## religion-atheism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2424 documents\n",
      "3 categories\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((array([0, 1]), array([1796,  628])), (2424, 7829))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "data = fetch_20newsgroups(subset='all', shuffle=False, remove=('headers', 'footers', 'quotes'), categories=['soc.religion.christian', 'talk.religion.misc', 'alt.atheism'])\n",
    "X, y = data.data, data.target\n",
    "print(\"%d documents\" % len(data.filenames))\n",
    "print(\"%d categories\" % len(data.target_names))\n",
    "\n",
    "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8, stop_words='english')\n",
    "vectorizer = Pipeline([\n",
    "    ('vect', CountVectorizer(**vectorizer_params)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "])\n",
    "X = vectorizer.fit_transform(X)\n",
    "msq = y == 2\n",
    "y[msq] = 1\n",
    "y[~msq] = 0\n",
    "np.unique(y, return_counts=True), X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ids_train = []\n",
    "ids_test = []\n",
    "\n",
    "for i in range(20):\n",
    "    id_train, id_test = train_test_split(np.arange(len(y)), test_size=.3)\n",
    "    ids_train.append(id_train)\n",
    "    ids_test.append(id_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([8, 2]))\n",
      "(array([0, 1]), array([6, 4]))\n",
      "(array([0, 1]), array([7, 3]))\n",
      "(array([0, 1]), array([9, 1]))\n",
      "(array([0, 1]), array([7, 3]))\n",
      "(array([0, 1]), array([7, 3]))\n",
      "(array([0, 1]), array([8, 2]))\n",
      "(array([0, 1]), array([6, 4]))\n",
      "(array([0, 1]), array([8, 2]))\n",
      "(array([0, 1]), array([9, 1]))\n",
      "(array([0, 1]), array([6, 4]))\n",
      "(array([0, 1]), array([8, 2]))\n",
      "(array([0, 1]), array([6, 4]))\n",
      "(array([0, 1]), array([7, 3]))\n",
      "(array([0, 1]), array([7, 3]))\n",
      "(array([0, 1]), array([8, 2]))\n",
      "(array([0, 1]), array([8, 2]))\n",
      "(array([0, 1]), array([7, 3]))\n",
      "(array([0, 1]), array([8, 2]))\n",
      "(array([0, 1]), array([6, 4]))\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(np.unique(y[ids_train[i][:10]], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('splits/trsplitrel_ath', ids_train)\n",
    "np.savez_compressed('splits/tstsplitrel_ath', ids_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digits dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([0, 1]), array([177, 182])), (359, 64))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "\n",
    "data = load_digits()\n",
    "X, y = data.data, data.target\n",
    "msq = np.logical_or(y == 1, y == 2)\n",
    "y = y[msq]\n",
    "y[y == 1] = 1\n",
    "y[y == 2] = 0\n",
    "X = X[msq]\n",
    "np.unique(y, return_counts=True), X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(251, 108)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_train, id_test = train_test_split(np.arange(len(y)), test_size=.3)\n",
    "len(id_train), len(id_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ids_train = []\n",
    "ids_test = []\n",
    "\n",
    "for i in range(20):\n",
    "    id_train, id_test = train_test_split(np.arange(len(y)), test_size=.3)\n",
    "    ids_train.append(id_train)\n",
    "    ids_test.append(id_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([8, 2]))\n",
      "(array([0, 1]), array([3, 7]))\n",
      "(array([0, 1]), array([6, 4]))\n",
      "(array([0, 1]), array([4, 6]))\n",
      "(array([0, 1]), array([7, 3]))\n",
      "(array([0, 1]), array([7, 3]))\n",
      "(array([0, 1]), array([7, 3]))\n",
      "(array([0, 1]), array([3, 7]))\n",
      "(array([0, 1]), array([7, 3]))\n",
      "(array([0, 1]), array([3, 7]))\n",
      "(array([0, 1]), array([4, 6]))\n",
      "(array([0, 1]), array([3, 7]))\n",
      "(array([0, 1]), array([5, 5]))\n",
      "(array([0, 1]), array([4, 6]))\n",
      "(array([0, 1]), array([3, 7]))\n",
      "(array([0, 1]), array([5, 5]))\n",
      "(array([0, 1]), array([4, 6]))\n",
      "(array([0, 1]), array([6, 4]))\n",
      "(array([0, 1]), array([6, 4]))\n",
      "(array([0, 1]), array([8, 2]))\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(np.unique(y[ids_train[i][:10]], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('splits/trsplitone_two', ids_train)\n",
    "np.savez_compressed('splits/tstsplitone_two', ids_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## odd_even"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([906, 891]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "data = load_digits()\n",
    "X, y = data.data, data.target\n",
    "msq = np.logical_or(np.logical_or(np.logical_or(np.logical_or(y == 0, y == 2), y == 4), y == 6), y == 8)\n",
    "y[msq] = 1\n",
    "y[~msq] = 0\n",
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ids_train = []\n",
    "ids_test = []\n",
    "\n",
    "for i in range(20):\n",
    "    id_train, id_test = train_test_split(np.arange(len(y)), test_size=.3)\n",
    "    ids_train.append(id_train)\n",
    "    ids_test.append(id_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([5, 5]))\n",
      "(array([0, 1]), array([9, 1]))\n",
      "(array([0, 1]), array([5, 5]))\n",
      "(array([0, 1]), array([6, 4]))\n",
      "(array([0, 1]), array([6, 4]))\n",
      "(array([0, 1]), array([4, 6]))\n",
      "(array([0, 1]), array([3, 7]))\n",
      "(array([0, 1]), array([8, 2]))\n",
      "(array([0, 1]), array([4, 6]))\n",
      "(array([0, 1]), array([3, 7]))\n",
      "(array([0, 1]), array([4, 6]))\n",
      "(array([0, 1]), array([5, 5]))\n",
      "(array([0, 1]), array([6, 4]))\n",
      "(array([0, 1]), array([5, 5]))\n",
      "(array([0, 1]), array([3, 7]))\n",
      "(array([0, 1]), array([6, 4]))\n",
      "(array([0, 1]), array([4, 6]))\n",
      "(array([0, 1]), array([4, 6]))\n",
      "(array([0, 1]), array([3, 7]))\n",
      "(array([0, 1]), array([4, 6]))\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(np.unique(y[ids_train[i][:10]], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('splits/trsplitodd_even', ids_train)\n",
    "np.savez_compressed('splits/tstsplitodd_even', ids_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
